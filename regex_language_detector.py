# -*- coding: utf-8 -*-
"""Regex Language Detector

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1zOrH7azTklchQG-u-VhZdqiEpuxC9s4c
"""

import pandas as pd
dados_portugues = pd.read_csv("stackoverflow_portugues.csv")
dados_portugues.head()

questao_portugues = dados_portugues.Questão[5]
print(questao_portugues)

dados_ingles = pd.read_csv("stackoverflow_ingles.csv")
dados_ingles.head()

questao_ingles = dados_ingles.Questão[0]
print(questao_ingles)

dados_espanhol = pd.read_csv("stackoverflow_espanhol.csv", sep=';', encoding="latin1")
dados_espanhol.head()

questao_espanhol = dados_espanhol.Questão[0]
print(questao_espanhol)

import re 
re.findall(r"<.*?>",questao_espanhol) #função da bibliotece de regex para encontrar expressões regulares

texto_teste = re.sub(r"<.*?>"," T----E----X-----T-----E",questao_portugues) #substituir as tags HTML que estão entre "<>" pela palavra teste usando re.sub
print(texto_teste)

def remover(textos, regex): #função para remover as tags HTML a partir da definição de uma regex pelo "re.compile"
  if type(textos) == str: 
    return regex.sub("",textos)
  else: 
      return [regex.sub("",texto) for texto in textos]

regex_html = re.compile(r"<.*?>")
questao_sem_tag = remover(questao_portugues,regex_html)
print(questao_sem_tag)

def substituir_codigo(textos, regex): #função para remover os trechos de código a partir da definição de uma regex pelo "re.compile"
  if type(textos) == str: 
    return regex.sub("CODE",textos)
  else: 
      return [regex.sub("CODE",texto) for texto in textos]

regex_codigo = re.compile(r"<code>(.|(\n))*?</code>") #Todos os trechos de códigos são iniciados com <code> e finalizados com </code>, contendo o caractere de quebra de linha \n
questao_sem_codigo = substituir_codigo(questao_ingles,regex_codigo)
print(questao_sem_codigo)

"""Remoção dos trechos de código e tags do arquivo em inglês """

questao_sem_codigo_tag = substituir_codigo(dados_ingles.Questão,regex_codigo) #chama função sub para retirar os trechos de código
dados_ingles["sem_codigo_tag"]=remover(questao_sem_codigo,regex_html)#chama a função remov para tirar tags HTML considerando o código já removido
dados_ingles.head()

"""Remoção dos trechos de código e tags do arquivo em português """

questao_sem_codigo_tag = substituir_codigo(dados_portugues.Questão,regex_codigo) #chama função sub para retirar os trechos de código
dados_portugues["sem_codigo_tag"]=remover(questao_sem_codigo,regex_html)#chama a função remov para tirar tags HTML considerando o código já removido
dados_portugues.head()

"""Remoção dos trechos de código e tags do arquivo em espanhol """

questao_sem_codigo_tag = substituir_codigo(dados_espanhol.Questão,regex_codigo) #chama função sub para retirar os trechos de código
dados_espanhol["sem_codigo_tag"]=remover(questao_sem_codigo,regex_html)#chama a função remov para tirar tags HTML considerando o código já removido
dados_espanhol.head()

questoes_port_sem_code = substituir_codigo(dados_portugues.Questão,
                                           regex_codigo)
questoes_port_sem_code_tag = remover(questoes_port_sem_code, regex_html)


dados_portugues["sem_code_tag"] = questoes_port_sem_code_tag

questoes_esp_sem_code = substituir_codigo(dados_espanhol.Questão,
                                           regex_codigo)
questoes_esp_sem_code_tag = remover(questoes_esp_sem_code, regex_html)


dados_espanhol["sem_code_tag"] = questoes_esp_sem_code_tag
dados_espanhol.head()

"""Remoção tags e código dos dados em português """

questoes_port_sem_code = substituir_codigo(dados_portugues.Questão,
                                           regex_codigo)
questoes_port_sem_code_tag = remover(questoes_port_sem_code, regex_html)


dados_portugues["sem_code_tag"] = questoes_port_sem_code_tag

"""Remoção tags e código dos dados em inglês """

questoes_ing_sem_code = substituir_codigo(dados_ingles.Questão,
                                           regex_codigo)
questoes_ing_sem_code_tag = remover(questoes_ing_sem_code, regex_html)


dados_ingles["sem_code_tag"] = questoes_ing_sem_code_tag

"""Remoção tags e código dos dados em espanhol"""

questoes_esp_sem_code = substituir_codigo(dados_espanhol.Questão,
                                           regex_codigo)
questoes_esp_sem_code_tag = remover(questoes_esp_sem_code, regex_html)


dados_espanhol["sem_code_tag"] = questoes_esp_sem_code_tag

"""Remover as pontuações da coluna sem código e tag HTML"""

regex_pontuacao = re.compile(r"[^\w\s]")
print(remover(questoes_esp_sem_code_tag[0], regex_pontuacao))

def minusculo(textos):
    if type(textos) == str:
        return textos.lower()
    else:
        return [texto.lower() for texto in textos]
print(minusculo(questoes_esp_sem_code_tag[0]))

regex_digitos = re.compile(r"\d+")
print(remover("Alura \n 1234 Caelum 1234", regex_digitos))

regex_espaco = re.compile(r" +")
regex_quebra_linha = re.compile(r"(\n)")
def substituir_por_espaco(textos, regex):
    if type(textos) == str:
        return regex.sub(" ", textos)
    else:
        return [regex.sub(" ", texto) for texto in textos]
    
print(substituir_por_espaco("Alura \n \n     Caleum", regex_quebra_linha))

"""Tratamento final dos dados em Português """

questoes_port_sem_pont = remover(dados_portugues.sem_code_tag, 
                                 regex_pontuacao)
questoes_port_sem_pont_minus = minusculo(questoes_port_sem_pont)
questoes_port_sem_pont_minus_dig = remover(questoes_port_sem_pont_minus,
                                          regex_digitos)


questoes_port_sem_quebra_linha = substituir_por_espaco(questoes_port_sem_pont_minus_dig,
                                                       regex_quebra_linha)
questoes_port_sem_espaco_duplicado = substituir_por_espaco(questoes_port_sem_quebra_linha,
                                                          regex_espaco)

dados_portugues["questoes_tratadas"] = questoes_port_sem_espaco_duplicado

"""Tratamento final dos dados em inglês """

questoes_ing_sem_pont = remover(dados_ingles.sem_code_tag, 
                                 regex_pontuacao)
questoes_ing_sem_pont_minus = minusculo(questoes_ing_sem_pont)
questoes_ing_sem_pont_minus_dig = remover(questoes_ing_sem_pont_minus,
                                          regex_digitos)


questoes_ing_sem_quebra_linha = substituir_por_espaco(questoes_ing_sem_pont_minus_dig,
                                                       regex_quebra_linha)
questoes_ing_sem_espaco_duplicado = substituir_por_espaco(questoes_ing_sem_quebra_linha,
                                                          regex_espaco)

dados_ingles["questoes_tratadas"] = questoes_ing_sem_espaco_duplicado

"""Tratamento final dos dados em espanhol """

questoes_esp_sem_pont = remover(dados_espanhol.sem_code_tag, 
                                 regex_pontuacao)
questoes_esp_sem_pont_minus = minusculo(questoes_esp_sem_pont)
questoes_esp_sem_pont_minus_dig = remover(questoes_esp_sem_pont_minus,
                                          regex_digitos)
questoes_esp_sem_quebra_linha = substituir_por_espaco(questoes_esp_sem_pont_minus_dig,
                                                       regex_quebra_linha)
questoes_esp_sem_espaco_duplicado = substituir_por_espaco(questoes_esp_sem_quebra_linha,
                                                          regex_espaco)
dados_espanhol["questoes_tratadas"] = questoes_esp_sem_espaco_duplicado

"""Criação do Modelo de Linguagem """

from nltk.util import bigrams #importar a função bigramas da biblioteca NLTK
texto_teste = "alura"
print(list(bigrams(texto_teste))) #Tuplas são os bigramas

!pip install nltk==3.5 #modificar a versão do NLTK disponível no Colab pra importar a lm.preprocessing
from nltk.lm.preprocessing import pad_both_ends
print(list(bigrams(pad_both_ends(texto_teste,n=2))))

"""Criando Coluna de verificação do idioma no Dataset """

dados_portugues["Idioma"] = "port"
dados_ingles["Idioma"] = "ing"
dados_espanhol["Idioma"] = "esp"

dados_portugues.head()

"""Criando o Modelo de ML para os dados em Português """

from sklearn.model_selection import train_test_split
port_treino,port_teste = train_test_split(dados_portugues.questoes_tratadas, test_size = 0.2, random_state = 123)

"""Criando o Modelo de ML para os dados em Inglês """

from sklearn.model_selection import train_test_split
ing_treino,ing_teste = train_test_split(dados_ingles.questoes_tratadas, test_size = 0.2, random_state = 123)

"""Criando o Modelo de ML para os dados em Espanhol """

from sklearn.model_selection import train_test_split
esp_treino,esp_teste = train_test_split(dados_espanhol.questoes_tratadas, test_size = 0.2, random_state = 123)

todas_questoes_port = ''.join(port_treino) # Unificar todas as questões em Português em uma única variável 
len(todas_questoes_port)

from nltk.tokenize import WhitespaceTokenizer
todas_palavras_port = WhitespaceTokenizer().tokenize(todas_questoes_port) #Tokenizar o join de todas as questões em port 
print(todas_palavras_port)
len(todas_palavras_port)

from nltk.lm.preprocessing import padded_everygram_pipeline #Função do NLTK para criação de bigramas ou unigramas 
port_treino_bigram,vocab_port = padded_everygram_pipeline(2,todas_palavras_port)

next(next(port_treino_bigram))

from nltk.lm import MLE

modelo_port = MLE(2)
modelo_port.fit(port_treino_bigram,vocab_port)

from nltk.lm import NgramCounter

modelo_port.counts[['m']].items() # Verificar a frequência de cada letra nos dados em português de treino

texto = "Good Morning"
palavras = WhitespaceTokenizer().tokenize(texto)
palavras_fakechar = [list(pad_both_ends(palavra,n=2)) for palavra in palavras] #Colocar o fake char em cada uma das palavras 
palavras_bigramns = [list(bigrams(palavra)) for palavra in palavras_fakechar] #Gerar bigramas a partir dos palavras com o fake char no início e no final 
print(palavras_fakechar)
print(palavras_bigramns)

print(modelo_port.perplexity(palavras_bigramns[0]))
print(modelo_port.perplexity(palavras_bigramns[1]))

def treinar_modelo_mle(lista_textos): 
  todas_questoes = ''.join(lista_textos)
  todas_palavras = WhitespaceTokenizer().tokenize(todas_questoes)
  bigrams, vocabulario = padded_everygram_pipeline(2,todas_palavras)
  modelo = MLE(2)
  modelo.fit(bigrams, vocabulario)

  return modelo

"""Testando o modelo para os dados em Português """

modelo_port_2 = treinar_modelo_mle(port_treino)
print(modelo_port_2.perplexity(palavras_bigramns[0]))
print(modelo_port_2.perplexity(palavras_bigramns[1]))

"""Testando o modelo para os dados em Inglês """

modelo_ing = treinar_modelo_mle(ing_treino)
print(modelo_ing.perplexity(palavras_bigramns[0]))
print(modelo_ing.perplexity(palavras_bigramns[1]))

"""Testando o modelo para os dados em Espanhol """

modelo_esp = treinar_modelo_mle(esp_treino)
print(modelo_esp.perplexity(palavras_bigramns[0]))
print(modelo_esp.perplexity(palavras_bigramns[1]))

def calcular_perplexidade(modelo,texto):

  perplexidade = 0 
  palavras = WhitespaceTokenizer().tokenize(texto)
  palavras_fakechar = [list(pad_both_ends(palavra,n=2)) for palavra in palavras] #Colocar o fake char em cada uma das palavras 
  palavras_bigramns = [list(bigrams(palavra)) for palavra in palavras_fakechar] #Gerar bigramas a partir dos palavras com o fake char no início e no final 

  for palavra in palavras_bigramns: 
    perplexidade += modelo.perplexity(palavra)

  return perplexidade

print(calcular_perplexidade(modelo_ing,"Good Morning"))

"""Definindo uma função para o Modelo de Laplace"""

from nltk.lm import Laplace

def treinar_modelo_Laplace(lista_textos): 
  todas_questoes = ''.join(lista_textos)
  todas_palavras = WhitespaceTokenizer().tokenize(todas_questoes)
  bigrams, vocabulario = padded_everygram_pipeline(2,todas_palavras)
  modelo = Laplace(2)
  modelo.fit(bigrams, vocabulario)

  return modelo

texto = "How are you"
palavras = WhitespaceTokenizer().tokenize(texto)
palavras_fakechar = [list(pad_both_ends(palavra,n=2)) for palavra in palavras] #Colocar o fake char em cada uma das palavras 
palavras_bigramns = [list(bigrams(palavra)) for palavra in palavras_fakechar] #Gerar bigramas a partir dos palavras com o fake char no início e no final 
print(palavras_fakechar)
print(palavras_bigramns)

"""Calculando perplexidade usando Laplace do modelo em Português """

modelo_port_Laplace = treinar_modelo_Laplace(port_treino)
print(modelo_port_Laplace.perplexity(palavras_bigramns[0]))
print(modelo_port_Laplace.perplexity(palavras_bigramns[1]))

"""Calculando perplexidade usando Laplace do modelo em Inglês"""

modelo_ing_Laplace = treinar_modelo_Laplace(ing_treino)
print(modelo_ing_Laplace.perplexity(palavras_bigramns[0]))
print(modelo_ing_Laplace.perplexity(palavras_bigramns[1]))

"""Calculando perplexidade usando Laplace do modelo em Espanhol """

modelo_esp_Laplace = treinar_modelo_Laplace(esp_treino)
print(modelo_esp_Laplace.perplexity(palavras_bigramns[0]))
print(modelo_esp_Laplace.perplexity(palavras_bigramns[1]))

"""Função de Atribuição de Idioma"""

def atribui_idioma(lista_textos): 
  idioma = []
  for texto in lista_textos: 
    portugues = calcular_perplexidade(modelo_port_Laplace, texto)
    ingles = calcular_perplexidade(modelo_ing_Laplace,texto)
    espanhol = calcular_perplexidade(modelo_esp_Laplace,texto)

    if ingles >= portugues <= espanhol: 
      idioma.append("Portugues")
    elif portugues > ingles < espanhol: 
      idioma.append("Ingles")
    else: 
      idioma.append("Espanhol")

  return idioma

resultados_portugues = atribui_idioma(port_teste)
print(resultados_portugues)

resultados_ingles = atribui_idioma(ing_teste)
print(resultados_ingles)

resultados_espanhol = atribui_idioma(esp_teste)
print(resultados_espanhol)

taxa_port = resultados_portugues.count("Portugues")/len(resultados_portugues)
taxa_ing = resultados_ingles.count("Ingles")/len(resultados_ingles)
taxa_esp = resultados_espanhol.count("Espanhol")/len(resultados_espanhol)

print("A taxa de acerto em português é de", 100*taxa_port,"%")
print("A taxa de acerto em Inglês é de", 100*taxa_ing,"%")
print("A taxa de acerto em Espanhol é de", 100*taxa_esp,"%")